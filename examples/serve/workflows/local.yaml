#
# Direct Workflow with Local MistralRs Model
#
# This example shows how to use local models with MistralRs backend
#
# For HuggingFace models:
#   - Set 'source' to the HuggingFace repo ID
#   - Use 'quant' for ISQ quantization (q4, q8, etc.)
#   - Optionally set 'model_type' to "text" or "vision"
#
# For local GGUF models:
#   - Set 'model_dir' in parameters to the directory containing GGUF files
#   - Use 'quant' to specify quantization type (q4_k_m, q8_0, etc.)
#
kind: Direct
name: LocalMathAgent
stream: false
description: "Math agent using local MistralRs model"

memory_persistence:
  mode: "memory"

workflow:
  agent:
    name: MathAgent
    description: "A helpful math calculator agent"
    instructions: |
      You are a math expert. Solve the given math problem step by step.
      Provide a clear and concise answer.
    executor: Basic
    memory:
      kind: sliding_window
      parameters:
        window_size: 10
    model:
      kind: llm
      preload: true
      backend:
        kind: local
      provider: mistral-rs
      # For HuggingFace model - uncomment and set repo ID:
      source: "microsoft/Phi-3.5-mini-instruct"
      # For local GGUF model - uncomment and set directory:
      # (comment out 'source' if using GGUF)
      # model_dir: "/path/to/gguf/model"
      parameters:
        # ISQ quantization for HF models (q4, q8, etc.)
        # or GGUF quant type (q4_k_m, q8_0, etc.)
        quant: "q8"
        model_type: "text"  # "text" or "vision"
        temperature: 0.1
        max_tokens: 500
        paged_attention: false
        verbose: true
        # Accelerator: cuda, metal, or cpu
        accelerator: "cuda"
    tools: [ ]
    output:
      type: text
  output:
    type: text
