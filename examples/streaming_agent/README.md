# Agent Streaming Support

This document outlines the real-time streaming feature implemented for `RunnableAgent` in the AutoAgents framework.

## Overview

The streaming feature allows an agent to return its response as a continuous stream of tokens, similar to how large language models like ChatGPT provide answers. Instead of waiting for the entire response to be generated, the user receives it piece by piece, providing a much more interactive and real-time experience.

## How It Works

The implementation relies on asynchronous streams in Rust.

1.  **`RunnableAgent::stream`:** The `RunnableAgent` trait now has a `stream` method that takes a `Task` and returns an asynchronous stream.
2.  **`Event::Token`:** A new `Event::Token(String)` variant was added to the `protocol`. The stream yields these events as new tokens are generated by the underlying language model.
3.  **Client-Side Processing:** The client (in our case, the `streaming_agent` example) can then iterate over this stream, processing each token as it arrives.

## How to Use the Example

To see the streaming feature in action, you can run the `streaming_agent` example.

### 1. Environment Setup

Before running the example, you need to provide your OpenAI API key.

-   Create a file named `.env` in the root of the project.
-   Add your API key to the file in the following format:
    ```
    OPENAI_API_KEY=your_api_key_here
    ```

This file is already included in `.gitignore` to prevent your key from being committed to version control.

### 2. Running the Example

A convenience script, `run.sh`, has been created to handle loading the environment variables and running the example.

From the project's root directory, simply run:

```bash
./run.sh
```

You will see the agent's response printed to the console token by token in real-time.
